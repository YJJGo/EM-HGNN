DBLP:
  lr: 0.001
  wd: 0
  attn_lr: 0.004
  attn_wd: 0
  epoch: 200
  embed_dim: 512
  hidden: 512
  num_hops: 4
  input_drop: 0.5
  dropout: 0.5
  out_layers: 3
  alpha: 0.3
  threshold1: 0.0
  threshold2: 0.0
  ratio: 0.4,0.9
  residual: True
  redundancy_module: True
  sim_module: True
  attention_module: True
  patience: 10

ACM:
  lr: 0.001
  wd: 0
  attn_lr: 0.004
  attn_wd: 0
  epoch: 200
  embed_dim: 512
  hidden: 512
  num_hops: 5
  input_drop: 0.5
  dropout: 0.5
  out_layers: 1
  alpha: 0.4
  threshold1: 0.6
  threshold2: 0.8
  ratio: 0.6,0.9
  residual: False
  redundancy_module: True
  sim_module: True
  attention_module: True
  patience: 30

IMDB:
  lr: 5e-5
  wd: 0
  attn_lr: 1e-4
  attn_wd: 0
  epoch: 400
  embed_dim: 512
  hidden: 512
  num_hops: 4
  input_drop: 0.0
  dropout: 0.5
  out_layers: 4
  alpha: 0.6
  threshold1: 0.3
  threshold2: 0.0
  ratio: 0.4,0.9
  residual: False
  redundancy_module: True
  sim_module: True
  attention_module: True
  patience: 30

AMiner:
  lr: 0.001
  wd: 0
  attn_lr: 0.001
  attn_wd: 0
  epoch: 200
  embed_dim: 512
  hidden: 512
  num_hops: 7
  input_drop: 0.5
  dropout: 0.5
  out_layers: 1
  alpha: 0.05
  threshold1: 0.0
  threshold2: 0.85
  ratio: 0.6,0.9
  residual: True
  redundancy_module: True
  sim_module: True
  attention_module: True
  batch_size: 128
  patience: 30
